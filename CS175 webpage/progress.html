<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<!--
Design by TEMPLATED
http://templated.co
Released for free under the Creative Commons Attribution License

Name       : GrassyGreen 
Description: A two-column, fixed-width design with dark color scheme.
Version    : 1.0
Released   : 20140310

-->
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>Solving Mazes in Minecraft Using Deep Reinforcement Learning</title>
<meta name="keywords" content="" />
<meta name="description" content="" />
<link href="http://fonts.googleapis.com/css?family=Raleway:400,200,500,600,700,800,300" rel="stylesheet" />
<link href="default.css" rel="stylesheet" type="text/css" media="all" />
<link href="fonts.css" rel="stylesheet" type="text/css" media="all" />
<!--[if IE 6]>
<link href="default_ie6.css" rel="stylesheet" type="text/css" />
<![endif]-->
</head>
<body>
<div id="wrapper">
	<div id="menu-wrapper">
		<div id="menu" class="container">
			<ul>
				<li><a href="index.html">Proposal</a></li>
				<li class="current_page_item"><a href="#">Progress Report</a></li>
				<li><a href="final.html">Final Report</a></li>
				<li><a href="about.html">About</a></li>
			</ul>
		</div>
		<!-- end #menu --> 
	<div id="header-wrapper">
		<div id="header" class="container">
			<div id="logo" style = "height: unset; width: unset;">
				<h1><a href="#">Solving Mazes in Minecraft Using Deep Reinforcement Learning</a></h1>
				<p>Project for UCI CS175: Project in Artificial Intelligence (Winter 2020)</p>
			</div>
		</div>
	</div>
	</div>
	<div id="banner" style = "background: url(images/progressbanner.jpg) no-repeat center;background-size: cover;"></div>
	<div id="page" class="container">
		<div id="content">
			<div class="title">
				<h2>Abstract</h2>
			</div>
			<p>Our project is to develop and train an agent that can guide our Minecraft user from the beginning of a maze to the end. The main idea is to create an agent that is capable of learning and familiarizing itself in different environments and then being able to make the best decision at every move to find the best overall route.  The agent will decide between simple actions such as moving as well as complicated actions like digging or building to reach the end of the maze. So far we have accomplished the minimum goal: to accomplish simple movement from the start point to the end point in a basic maze (small in size and not difficult to navigate). Using deep Q-learning, the agent showed capability in solving basic mazes efficiently by finding the fastest path to the end of the maze.<br><br></p>


			<div class="title">
				<h2>1  Introduction</h2>
			</div>
			<p>The goal of our project is to train our agent to be able to navigate a maze from beginning to end. We want our agent to be able to learn to direct the user through varying obstacles based on its experience amassed from reinforced learning. What separates an artificial intelligence agent from just a well-coded maze solver is its ability to learn and adapt and make good decisions in unknown environments. We wanted to create an agent that isn’t just good at solving mazes through countless repeated runs because any computer program could do that.  We wanted an agent that can make smart decisions at every move and find the optimal route to navigate itself through an unfamiliar maze without running it an unlimited number of times. We decided to implement a deep Q-learning approach because it addresses both of these demands. Q-learning is interesting because it implements randomization during the testing phase to force the algorithm to deal with unpredictable states and require a degree of generalization when it comes to decision making.  This answers our problem as implementing Q-learning allows our agent to rely on reinforced learning to be able to build up enough experience to make smart decisions when placed in a foreign maze. We played around with different approaches of using grid-based observations using tools provided by the Malmo platform versus frame-based observations where each frame of the game represented a different state and is visual dependent as compared to the frame-based approach. At the moment, we have completed our minimum goal using both approaches.  Continuing on to our second milestone, we will continue to analyze trade offs and make a decision on which approach to stick with as we continue to try to complete more complex objectives.<br><br>
			</p>

			<div class="title">
				<h2>2  Background</h2>
			</div>
			<p>Our project is set in Minecraft, a video game that allows a player to control an agent to explore and craft in the world, and build complicated structures. We are using Project Malmo, an open source platform built on Minecraft for artificial intelligence experimentation to help the agent act and sense within the Minecraft environment. We are also using its provided Q-learning library.<br><br>
			</p>

			<div class="title">
				<h2>3  Problem Statement</h2>
			</div>
			<p>The minimum goal of our project was to accomplish simple movement from the start point to the end point in a basic maze.  The first milestone was using only back and forth movement and then in the second milestone, use a combination of front, back, left, and right movement to navigate to the end point. The realistic goal will be to add in jumping/digging actions in order for the agent to achieve its goal. The first milestone will be to teach the agent to learn to dig when able. The second milestone will be to teach the agent to dig and make paths when it is more efficient to do so, rather than exploring and using a longer path.  The ambitious goal will be to allow the agent to build in the maze to help it reach the end point. The agent should learn whether it would be more efficient to build and jump over a wall rather than going around the maze walls. For this goal, its milestone would be to build to jump over a wall or dig under the wall rather than going around the wall to get to the end point.<br><br>
			</p>

			<div class="title">
				<h2>4  Method</h2>
			</div>
			<p>Our agent follows the Markov Decision Process to determine the most efficient path. At each step, the agent examines and interacts with the environment and receives a certain reward.  With each action, the agent receives a reward that depends on all previous states and actions it took to get to the current state. Thus, the agent chooses the appropriate action for its current state depending on actions in previous states. Now given we know the expected reward of each action at every step, we use Q-learning, to generate the maximum total reward, which in Q-learning is called the Q-value. Q(s, a) =r(s, a) + maxaQ(s′, a) The Q-value we calculate comes from being at the state “s” at which we will make the action “a”. To get Q, we will calculate the immediate reward of making action “a” at state “s”, shown by “r(s,a).” We will add this with the highest Q-value possible from the next state s’. We include the value of gamma as it controls how much influence the future reward has by either increasing or decreasing it based on what value we assert gamma as. We also implement anε-greedy policy to avoid getting stuck in the local minimum if it only chooses actions that optimize the Q-function.  Theε-greedy method will either choose the action with the highest q-value or a random action.  This decision is made randomly and is based on the value of epsilon where the value ofεis purposely set so that we lean towards using random actions at first, but as training progresses, become more reliant on maximum Q-values instead. With this policy, the agent will randomly choose an action that is not optimal so the agent has the chance to explore the environment more at the off chance that the true optimal path is elsewhere and prevent overfitting.<br><br>
			</p>

			<div class="title">
				<h2>References</h2>
			</div>
			<p>[1] Choudhary, Ankit, et al.  “Introduction to Deep Q-Learning for Reinforcement Learning (in Python).”Analytics Vidhya, 6 May 2019, www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python/.
			</p>
		</div>

		<div id="sidebar">
			<div class="box2">
			</div>
		</div>
	</div>
</div>
	
<div id="copyright" class="container">
	<p>&copy; Untitled. All rights reserved. | Photos by <a href="http://fotogrph.com/">Fotogrph</a> | Design by <a href="http://templated.co" rel="nofollow">TEMPLATED</a>.</p>
</div>
</body>
</html>
